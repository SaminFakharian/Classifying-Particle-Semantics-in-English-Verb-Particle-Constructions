{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:44:59.315351Z",
     "start_time": "2020-04-08T00:44:59.302359Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import cross_val_score, StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import utils\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:45:00.742282Z",
     "start_time": "2020-04-08T00:45:00.738284Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports needed and logging\n",
    "import gzip\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the BNC written data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:45:03.864022Z",
     "start_time": "2020-04-08T00:45:03.841036Z"
    },
    "code_folding": [
     13
    ]
   },
   "outputs": [],
   "source": [
    "# Natural Language Toolkit: Plaintext Corpus Reader\n",
    "#\n",
    "# Copyright (C) 2001-2019 NLTK Project\n",
    "# Author: Edward Loper <edloper@gmail.com>\n",
    "# URL: <http://nltk.org/>\n",
    "# For license information, see LICENSE.TXT\n",
    "\n",
    "\"\"\"Corpus reader for the XML version of the British National Corpus.\"\"\"\n",
    "\n",
    "from nltk.corpus.reader.util import concat\n",
    "from nltk.corpus.reader.xmldocs import XMLCorpusReader, XMLCorpusView, ElementTree\n",
    "\n",
    "\n",
    "class BNCCorpusReader(XMLCorpusReader):\n",
    "    \"\"\"Corpus reader for the XML version of the British National Corpus.\n",
    "\n",
    "    For access to the complete XML data structure, use the ``xml()``\n",
    "    method.  For access to simple word lists and tagged word lists, use\n",
    "    ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.\n",
    "\n",
    "    You can obtain the full version of the BNC corpus at\n",
    "    http://www.ota.ox.ac.uk/desc/2554\n",
    "\n",
    "    If you extracted the archive to a directory called `BNC`, then you can\n",
    "    instantiate the reader as::\n",
    "\n",
    "        BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, fileids, lazy=True):\n",
    "        XMLCorpusReader.__init__(self, root, fileids)\n",
    "        self._lazy = lazy\n",
    "\n",
    "    def words(self, fileids=None, strip_space=True, stem=True):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of words\n",
    "            and punctuation symbols.\n",
    "        :rtype: list(str)\n",
    "\n",
    "        :param strip_space: If true, then strip trailing spaces from\n",
    "            word tokens.  Otherwise, leave the spaces on the tokens.\n",
    "        :param stem: If true, then use word stems instead of word strings.\n",
    "        \"\"\"\n",
    "        return self._views(fileids, False, None, strip_space, stem)\n",
    "\n",
    "    def tagged_words(self, fileids=None, c5=True, strip_space=True, stem=True):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of tagged\n",
    "            words and punctuation symbols, encoded as tuples\n",
    "            ``(word,tag)``.\n",
    "        :rtype: list(tuple(str,str))\n",
    "\n",
    "        :param c5: If true, then the tags used will be the more detailed\n",
    "            c5 tags.  Otherwise, the simplified tags will be used.\n",
    "        :param strip_space: If true, then strip trailing spaces from\n",
    "            word tokens.  Otherwise, leave the spaces on the tokens.\n",
    "        :param stem: If true, then use word stems instead of word strings.\n",
    "        \"\"\"\n",
    "        tag = 'c5' if c5 else 'pos'\n",
    "        return self._views(fileids, False, tag, strip_space, stem)\n",
    "\n",
    "    def sents(self, fileids=None, strip_space=True, stem=True):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of\n",
    "            sentences or utterances, each encoded as a list of word\n",
    "            strings.\n",
    "        :rtype: list(list(str))\n",
    "\n",
    "        :param strip_space: If true, then strip trailing spaces from\n",
    "            word tokens.  Otherwise, leave the spaces on the tokens.\n",
    "        :param stem: If true, then use word stems instead of word strings.\n",
    "        \"\"\"\n",
    "        return self._views(fileids, True, None, strip_space, stem)\n",
    "\n",
    "    def tagged_sents(self, fileids=None, c5=True, strip_space=True, stem=True):\n",
    "        \"\"\"\n",
    "        :return: the given file(s) as a list of\n",
    "            sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
    "        :rtype: list(list(tuple(str,str)))\n",
    "\n",
    "        :param c5: If true, then the tags used will be the more detailed\n",
    "            c5 tags.  Otherwise, the simplified tags will be used.\n",
    "        :param strip_space: If true, then strip trailing spaces from\n",
    "            word tokens.  Otherwise, leave the spaces on the tokens.\n",
    "        :param stem: If true, then use word stems instead of word strings.\n",
    "        \"\"\"\n",
    "        tag = 'c5' if c5 else 'pos'\n",
    "        return self._views(\n",
    "            fileids, sent=True, tag=tag, strip_space=strip_space, stem=stem\n",
    "        )\n",
    "\n",
    "    def _views(self, fileids=None, sent=False, tag=False, strip_space=True, stem=True):\n",
    "        \"\"\"A helper function that instantiates BNCWordViews or the list of words/sentences.\"\"\"\n",
    "        f = BNCWordView if self._lazy else self._words\n",
    "        return concat(\n",
    "            [\n",
    "                f(fileid, sent, tag, strip_space, stem)\n",
    "                for fileid in self.abspaths(fileids)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _words(self, fileid, bracket_sent, tag, strip_space, stem):\n",
    "        \"\"\"\n",
    "        Helper used to implement the view methods -- returns a list of\n",
    "        words or a list of sentences, optionally tagged.\n",
    "\n",
    "        :param fileid: The name of the underlying file.\n",
    "        :param bracket_sent: If true, include sentence bracketing.\n",
    "        :param tag: The name of the tagset to use, or None for no tags.\n",
    "        :param strip_space: If true, strip spaces from word tokens.\n",
    "        :param stem: If true, then substitute stems for words.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        xmldoc = ElementTree.parse(fileid).getroot()\n",
    "        for xmlsent in xmldoc.findall('.//s'):\n",
    "            sent = []\n",
    "            for xmlword in _all_xmlwords_in(xmlsent):\n",
    "                word = xmlword.text\n",
    "                if not word:\n",
    "                    word = \"\"  # fixes issue 337?\n",
    "                if strip_space or stem:\n",
    "                    word = word.strip()\n",
    "                if stem:\n",
    "                    word = xmlword.get('hw', word)\n",
    "                if tag == 'c5':\n",
    "                    word = (word, xmlword.get('c5'))\n",
    "                elif tag == 'pos':\n",
    "                    word = (word, xmlword.get('pos', xmlword.get('c5')))\n",
    "                sent.append(word)\n",
    "            if bracket_sent:\n",
    "                result.append(BNCSentence(xmlsent.attrib['n'], sent))\n",
    "            else:\n",
    "                result.extend(sent)\n",
    "\n",
    "        assert None not in result\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:45:05.711858Z",
     "start_time": "2020-04-08T00:45:05.704862Z"
    },
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def _all_xmlwords_in(elt, result=None):\n",
    "    if result is None:\n",
    "        result = []\n",
    "    for child in elt:\n",
    "        if child.tag in ('w'):\n",
    "            result.append(child)\n",
    "        else:\n",
    "            _all_xmlwords_in(child, result)\n",
    "    return result\n",
    "\n",
    "\n",
    "class BNCSentence(list):\n",
    "    \"\"\"\n",
    "    A list of words, augmented by an attribute ``num`` used to record\n",
    "    the sentence identifier (the ``n`` attribute from the XML).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num, items):\n",
    "        self.num = num\n",
    "        list.__init__(self, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:45:06.080627Z",
     "start_time": "2020-04-08T00:45:06.055642Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BNCWordView(XMLCorpusView):\n",
    "    \"\"\"\n",
    "    A stream backed corpus view specialized for use with the BNC corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    tags_to_ignore = set(\n",
    "        ['stext','pb', 'gap', 'vocal', 'event', 'unclear', 'shift', 'pause', 'align','c']\n",
    "    )\n",
    "    \"\"\"These tags are ignored. For their description refer to the\n",
    "    technical documentation, for example,\n",
    "    http://www.natcorp.ox.ac.uk/docs/URG/ref-vocal.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fileid, sent, tag, strip_space, stem):\n",
    "        \"\"\"\n",
    "        :param fileid: The name of the underlying file.\n",
    "        :param sent: If true, include sentence bracketing.\n",
    "        :param tag: The name of the tagset to use, or None for no tags.\n",
    "        :param strip_space: If true, strip spaces from word tokens.\n",
    "        :param stem: If true, then substitute stems for words.\n",
    "        \"\"\"\n",
    "        if sent:\n",
    "            tagspec = '.*/s'\n",
    "        else:\n",
    "            tagspec = '.*/s/(.*/)?(w)'\n",
    "        self._sent = sent\n",
    "        self._tag = tag\n",
    "        self._strip_space = strip_space\n",
    "        self._stem = stem\n",
    "\n",
    "        self.title = None  #: Title of the document.\n",
    "        self.author = None  #: Author of the document.\n",
    "        self.editor = None  #: Editor\n",
    "        self.resps = None  #: Statement of responsibility\n",
    "\n",
    "        XMLCorpusView.__init__(self, fileid, tagspec)\n",
    "\n",
    "        # Read in a tasty header.\n",
    "        self._open()\n",
    "        self.read_block(self._stream, '.*/teiHeader$', self.handle_header)\n",
    "        self.close()\n",
    "\n",
    "        # Reset tag context.\n",
    "        self._tag_context = {0: ()}\n",
    "\n",
    "    def handle_header(self, elt, context):\n",
    "        # Set up some metadata!\n",
    "        titles = elt.findall('titleStmt/title')\n",
    "        if titles:\n",
    "            self.title = '\\n'.join(title.text.strip() for title in titles)\n",
    "\n",
    "        authors = elt.findall('titleStmt/author')\n",
    "        if authors:\n",
    "            self.author = '\\n'.join(author.text.strip() for author in authors)\n",
    "\n",
    "        editors = elt.findall('titleStmt/editor')\n",
    "        if editors:\n",
    "            self.editor = '\\n'.join(editor.text.strip() for editor in editors)\n",
    "\n",
    "        resps = elt.findall('titleStmt/respStmt')\n",
    "        if resps:\n",
    "            self.resps = '\\n\\n'.join(\n",
    "                '\\n'.join(resp_elt.text.strip() for resp_elt in resp) for resp in resps\n",
    "            )\n",
    "\n",
    "    def handle_elt(self, elt, context):\n",
    "        if self._sent:\n",
    "            return self.handle_sent(elt)\n",
    "        else:\n",
    "            return self.handle_word(elt)\n",
    "\n",
    "    def handle_word(self, elt):\n",
    "        word = elt.text\n",
    "        if elt.get('c5') not in ['PUL','PUN','PUQ','PUR']:\n",
    "            if not word:\n",
    "                word = \"\"  # fixes issue 337?\n",
    "            if self._strip_space or self._stem:\n",
    "                word = word.strip()\n",
    "            if self._stem:\n",
    "                word = elt.get('hw', word)\n",
    "            if self._tag == 'c5':\n",
    "                    word = (word, elt.get('c5'))\n",
    "            elif self._tag == 'pos':\n",
    "                word = (word, elt.get('pos', elt.get('c5')))\n",
    "            return word\n",
    "\n",
    "    def handle_sent(self, elt):\n",
    "        sent = []\n",
    "        for child in elt:\n",
    "            if child.tag in ('mw', 'hi', 'corr', 'trunc'):\n",
    "                sent += [self.handle_word(w) for w in child]\n",
    "            elif child.tag in ('w'):\n",
    "                sent.append(self.handle_word(child))\n",
    "            elif child.tag not in self.tags_to_ignore:\n",
    "                raise ValueError('Unexpected element %s' % child.tag)\n",
    "        return BNCSentence(elt.attrib['n'], sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:45:09.027610Z",
     "start_time": "2020-04-08T00:45:08.998607Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\\w*/\\w*\\.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:29:44.745236Z",
     "start_time": "2020-04-08T00:45:10.915674Z"
    }
   },
   "outputs": [],
   "source": [
    "tagged_sentences_wordforms=[]\n",
    "#we are having the root form of the words\n",
    "for word in parser.tagged_sents():\n",
    "    tagged_sentences_wordforms.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:29:44.849180Z",
     "start_time": "2020-04-08T01:29:44.787199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences_wordforms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:57:23.084046Z",
     "start_time": "2020-04-08T01:29:44.852162Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences=[]\n",
    "for s in tagged_sentences_wordforms:\n",
    "    sentence=[]\n",
    "    for w in s:\n",
    "        if w!= None:\n",
    "            sentence.append(w)\n",
    "    tagged_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:57:23.264194Z",
     "start_time": "2020-04-08T01:57:23.138028Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:57:24.598382Z",
     "start_time": "2020-04-08T01:57:23.285182Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences=[i for i in tagged_sentences if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T01:57:24.620370Z",
     "start_time": "2020-04-08T01:57:24.600380Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _VPC (just up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:16:44.441954Z",
     "start_time": "2020-04-08T01:57:24.629362Z"
    },
    "code_folding": [
     10
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identifying VPC\n",
    "word_particle = []\n",
    "# for l in word_verb:\n",
    "for l in tagged_sentences:\n",
    "    c = []\n",
    "    for w in l:\n",
    "        i = list(w)\n",
    "        start_word = i[1]\n",
    "        if start_word != None:\n",
    "            if start_word.startswith('V'):\n",
    "                flag = False\n",
    "                start = l.index(w)+1\n",
    "                end = l.index(w)+7\n",
    "                for n in range(start, end):\n",
    "                    if n < len(l):\n",
    "                        if l[n][1] != None:\n",
    "                            if l[n][1].startswith('V'):\n",
    "                                break\n",
    "                            elif l[n][1] == 'AVP' and l[n][0] == 'up':\n",
    "                                c.append(i[0]+'_VPC')\n",
    "                                flag = True\n",
    "                        else:\n",
    "                            continue\n",
    "                if not flag:\n",
    "                    c.append(i[0])\n",
    "    #                 c.append(i.split('_')[0]+'_V')\n",
    "\n",
    "            else:\n",
    "                c.append(i[0])\n",
    "        else:\n",
    "            continue\n",
    "    word_particle.append(c)\n",
    "\n",
    "word_particle[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:16:48.232177Z",
     "start_time": "2020-04-08T02:16:44.559294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_particle=[i for i in word_particle if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:23:29.841148Z",
     "start_time": "2020-04-08T02:23:14.381056Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in word_particle:\n",
    "    for j in i:\n",
    "        if j.find(\"_\") != -1:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:30:15.506222Z",
     "start_time": "2020-04-08T02:25:14.693953Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# particle features\n",
    "verb_particle_list = []\n",
    "for l in tagged_sentences:\n",
    "    for w in l:\n",
    "        i = list(w)\n",
    "        start_word = i[1]\n",
    "        if start_word != None:\n",
    "            if start_word.startswith('V'):\n",
    "                start = l.index(w)+1\n",
    "                end = l.index(w)+7\n",
    "                for n in range(start, end):\n",
    "                    if n < len(l):\n",
    "                        if l[n][1] != None:\n",
    "                            if l[n][1].startswith('V'):\n",
    "                                break\n",
    "                            elif l[n][1] == 'AVP' and l[n][0] == 'up':\n",
    "                                verb_particle_list.append(\n",
    "                                    (i[0], l[n][0], n-start))\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "verb_particle_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _VPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "# # identifying VPC\n",
    "# word_particle = []\n",
    "# # for l in word_verb:\n",
    "# for l in tagged_sentences:\n",
    "#     c = []\n",
    "#     for w in l:\n",
    "#         i = list(w)\n",
    "#         start_word = i[1]\n",
    "#         if start_word != None:\n",
    "#             if start_word.startswith('V'):\n",
    "#                 flag = False\n",
    "#                 start = l.index(w)+1\n",
    "#                 end = l.index(w)+7\n",
    "#                 for n in range(start, end):\n",
    "#                     if n < len(l):\n",
    "#                         if l[n][1] != None:\n",
    "#                             if l[n][1].startswith('V'):\n",
    "#                                 break\n",
    "#                             elif l[n][1] == 'AVP':\n",
    "#                                 c.append(i[0]+'_VPC')\n",
    "#                                 flag = True\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                 if not flag:\n",
    "#                     c.append(i[0])\n",
    "#     #                 c.append(i.split('_')[0]+'_V')\n",
    "\n",
    "#             else:\n",
    "#                 c.append(i[0])\n",
    "#         else:\n",
    "#             continue\n",
    "#     word_particle.append(c)\n",
    "\n",
    "# word_particle[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:16:48.232177Z",
     "start_time": "2020-04-08T02:16:44.559294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_particle=[i for i in word_particle if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:23:29.841148Z",
     "start_time": "2020-04-08T02:23:14.381056Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in word_particle:\n",
    "    for j in i:\n",
    "        if j.find(\"_\") != -1:\n",
    "            print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T00:44:46.849929Z",
     "start_time": "2020-04-08T00:44:03.423Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # particle features\n",
    "# verb_particle_list = []\n",
    "# for l in tagged_sentences:\n",
    "#     for w in l:\n",
    "#         i = list(w)\n",
    "#         start_word = i[1]\n",
    "#         if start_word != None:\n",
    "#             if start_word.startswith('V'):\n",
    "#                 start = l.index(w)+1\n",
    "#                 end = l.index(w)+7\n",
    "#                 for n in range(start, end):\n",
    "#                     if n < len(l):\n",
    "#                         if l[n][1] != None:\n",
    "#                             if l[n][1].startswith('V'):\n",
    "#                                 break\n",
    "#                             elif l[n][1] == 'AVP':\n",
    "#                                 verb_particle_list.append(\n",
    "#                                     (i[0], l[n][0], n-start))\n",
    "#                         else:\n",
    "#                             continue\n",
    "\n",
    "#             else:\n",
    "#                 continue\n",
    "#         else:\n",
    "#             continue\n",
    "# verb_particle_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(verb_particle_list, columns=['verb','particle','position'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_excel(\"verb_particle_list.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.particle.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.verb.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## did some feature engineering on excel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_features_df=pd.read_excel('particle_features.xlsx')\n",
    "# particle_features_df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "particle_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_features_df[['about', 'along', 'around', 'back', 'by', 'down', 'in', 'off',\n",
    "                          'on', 'out', 'over', 'round', 'through', 'under', 'up', 'up_0', 'up_1',\n",
    "                          'up_2', 'up_3', 'up_4', 'up_5']].to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "particle_features={}\n",
    "for index, row in particle_features_df.iterrows():\n",
    "    particle_features[particle_features_df.iloc[index]['verb']]=particle_features_df[['about', 'along', 'around', 'back', 'by', 'down', 'in', 'off',\n",
    "                          'on', 'out', 'over', 'round', 'through', 'under', 'up', 'up_0', 'up_1',\n",
    "                          'up_2', 'up_3', 'up_4', 'up_5']].to_numpy(dtype='float32')\n",
    "particle_features['fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_verb=[]\n",
    "# for l in d0:\n",
    "for l in tagged_sentences:\n",
    "    word_verb_tagged=[]\n",
    "    for t in l:\n",
    "        T = list(t)\n",
    "        if str(T[1]).startswith('V'):\n",
    "            T[0]=T[0]+'_V'\n",
    "        word_verb_tagged.append(T[0])\n",
    "    word_verb.append(word_verb_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training word2vec on BNC corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle = Word2Vec(word_particle, min_count=5,size= 300,workers=3, window =5,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle.save(\"model_BNC_tagged_sentences_particle_sg_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle = Word2Vec(word_particle, min_count=5,size= 300,workers=3, window =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle.save(\"model_BNC_tagged_sentences_particle_cw_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_verb = Word2Vec(word_verb, min_count=5,size= 300,workers=3, window =5,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_verb.save(\"model_BNC_tagged_sentences_verb_sg_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_verb = Word2Vec(word_verb, min_count=5,size= 300,workers=3, window =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_verb.save(\"model_BNC_tagged_sentences_verb_cw_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T03:01:42.194005Z",
     "start_time": "2020-04-08T02:36:43.026035Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle_up = Word2Vec(word_particle, min_count=5,size= 300,workers=3, window =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T03:01:49.550789Z",
     "start_time": "2020-04-08T03:01:42.227989Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle_up.save(\"model_BNC_tagged_sentences_particle_up_cw.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:16:57.646595Z",
     "start_time": "2020-04-08T03:01:49.555787Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle_up = Word2Vec(word_particle, min_count=5,size= 300,workers=3, window =5,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T04:16:59.795369Z",
     "start_time": "2020-04-08T04:16:57.706560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_BNC_tagged_sentences_particle_up.save(\"model_BNC_tagged_sentences_particle_up_sg.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_BNC_tagged_sentences_particle.save(\"model_BNC_tagged_sentences_particle_sg_window3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
